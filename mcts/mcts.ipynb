{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "from settings.constants import *\n",
    "from utils.funcs import *\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=OPENAI_API_BASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_request_openai(prompt: str = \"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=1.0,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    if response.choices:\n",
    "        return response.choices[0].message.content\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_draft_answer_critique(question, draft_answer):\n",
    "    prompt = (\n",
    "        f\"Question: {question}\"\n",
    "        f\"Draft Answer: {draft_answer}\"\n",
    "        \"Please critique the draft answer.\"\n",
    "        \"Carefully assess if the answer is correct of not and why.\"\n",
    "        \"Consider multiple ways of verifying if the answer is correct.\"\n",
    "        \"DO: Point out EVERY flaw and hold the draft answer to a very high standard.\"\n",
    "        \"DO: Provide specific recommendations to improve the answer.\"\n",
    "        \"DO: Think step by step.\"\n",
    "        \"DO NOT provide a revised answer\"\n",
    "    )\n",
    "    return chat_completion_request_openai(prompt)\n",
    "\n",
    "\n",
    "def get_improved_answer(question, draft_answer, critique):\n",
    "    prompt = (\n",
    "        f\"Question: {question}\"\n",
    "        f\"Draft Answer: {draft_answer}\"\n",
    "        f\"Critique: {critique}\"\n",
    "        \"Please improve the draft answer based on the critique following this format:\"\n",
    "        \"Reasoning Process: <step-by-ste- reasoning process>\\n\"\n",
    "        \"Verification: <verification of the facts>\\n\"\n",
    "        \"Final Answer: <the improved and verified answer>\\n\"\n",
    "    )\n",
    "    return chat_completion_request_openai(prompt)\n",
    "\n",
    "def rate_answer(question, answer):\n",
    "    prompt = (\n",
    "        f\"Question: {question}\"\n",
    "        f\"Answer: {answer}\"\n",
    "        \"As an expert on this topic, please provide a detailed critique \"\n",
    "        \"Provide ONLY a critique, not a suggested answer.\"\n",
    "        \"Then, rate the answer on a scale of 0 to 100.\"\n",
    "        \"The response should be in the following format:\"\n",
    "        \"Critique: <detailed critique>\\n\"\n",
    "        \"Rating: <0 to 100 rating>\\n\"\n",
    "    )\n",
    "    response = chat_completion_request_openai(prompt)\n",
    "    try: \n",
    "        match = re.search(r\"Rating:\\s+(\\d+)\", response)\n",
    "        if match:\n",
    "            rating = int(match.group(1))\n",
    "            if rating > 95:\n",
    "                rating = 95.0\n",
    "            rating /= 100.0\n",
    "        else:\n",
    "            raise ValueError(\"Rating not found in response.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting rating: {e}\")\n",
    "        print(f\"Rating response was: {response}\")\n",
    "        rating = 0.0\n",
    "    return rating\n",
    "\n",
    "\n",
    "def get_answer_directly_from_llm(question):\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Please provide the answer to this question with detailed reasoning. Follow this format:\\n\"\n",
    "        \"Reasoning process: <step-by-step reasoning process>\\n\"\n",
    "        \"Verification: <verification of facts>\\n\"\n",
    "        \"Final Answer: <the improved and verified answer>\\n\"\n",
    "    )\n",
    "    llm_response = chat_completion_request_openai(prompt)\n",
    "\n",
    "    try:\n",
    "        match = re.search(r\"Final Answer:\\s*(.*)\", llm_response)\n",
    "        final_answer = match.group(1).strip() if match else None\n",
    "    except Exception as e:\n",
    "        final_answer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is an example of how you can represent an array of strings where each array corresponds to a season:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"seasons\": {\n",
      "    \"spring\": [\"March\", \"April\", \"May\"],\n",
      "    \"summer\": [\"June\", \"July\", \"August\"],\n",
      "    \"fall\": [\"September\", \"October\", \"November\"],\n",
      "    \"winter\": [\"December\", \"January\", \"February\"]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "In this format, each key represents a season, and the value is an array of strings corresponding to the months typically associated with that season. If you are looking for something different, please let me know!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"return an array of strings where each array is a season\"\n",
    "response = chat_completion_request_openai(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The draft answer states, \"The capital of France is Lyon,\" which is incorrect. The accurate capital of France is Paris. Here are several points of critique regarding the draft answer:\n",
      "\n",
      "1. **Factual Error**: The most glaring flaw in the draft is the assertion that Lyon is the capital. This is incorrect information that misleads the reader. \n",
      "\n",
      "2. **Verification Method**: To ensure accuracy, one could verify this information in various reliable sources including:\n",
      "   - **Official Government Websites**: The French government’s website would list Paris as the capital.\n",
      "   - **Encyclopedias**: Reputable encyclopedias, such as Britannica or similar platforms, would confirm that Paris is the capital.\n",
      "   - **Geographical Texts**: Reference materials on geography or political science would also indicate Paris as the capital city.\n",
      "\n",
      "3. **Clarity and Precision**: The answer lacks clarity in explaining why Lyon is mentioned in the context of France. Lyon is a significant city known for its historical and cultural importance, but it does not serve as the capital. The draft should clarify this distinction.\n",
      "\n",
      "4. **Context**: Providing context around Paris as the capital city (historical significance, political relevance, and demography) would improve the answer significantly. Mentioning Lyon’s characteristics, while acknowledging it is not the capital, could provide a more rounded response.\n",
      "\n",
      "5. **Formatting and Presentation**: The draft presents the information in a straightforward manner but fails to include necessary qualifiers, like \"current capital\" or \"recognized capital.\" A high standard would require precise language.\n",
      "\n",
      "6. **Revisiting the Question**: The answer could benefit from directly addressing the original question more clearly. It should reaffirm the correct information about the capital without mingling it with incorrect content.\n",
      "\n",
      "In summary, the draft answer needs significant improvement in factual accuracy, clarity, context, and thoroughness. Recommendations would include rigorously verifying facts, providing contextual information, and ensuring that the answer is precise and clear.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France\"\n",
    "draft_answer = \"The capital of France is Lyon\"\n",
    "\n",
    "critique = get_draft_answer_critique(prompt, draft_answer)\n",
    "print(critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Process: The initial draft inaccurately identified Lyon as the capital of France. To correct this, I must first recall the correct capital city based on my knowledge of world geography and the status of cities in France. Paris is widely recognized as the capital and is the largest city in France, serving as the political, economic, and cultural center of the country.\n",
      "\n",
      "Verification: I can verify that Paris has been the capital of France since 508 AD, and it is frequently referenced in geography textbooks, government documents, and even travel guides. Additionally, information from reputable sources like encyclopedias and official government websites confirms that Paris is the capital.\n",
      "\n",
      "Final Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France\"\n",
    "draft_answer = \"The capital of France is Lyon\"\n",
    "critique = \"That is just wrong\"\n",
    "\n",
    "improved_answer = get_improved_answer(prompt, draft_answer, critique)\n",
    "print(improved_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating was: 0.7\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France\"\n",
    "answer = \"The capital of France is Paris\"\n",
    "rating = rate_answer(question, answer)\n",
    "print(f\"Rating was: {rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Improvement Rating Pipeline\n",
    "For a given anser to a question:\n",
    "1. Critique\n",
    "2. Improve\n",
    "3. Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_answers = [\n",
    "    \"I don't know the answer\",\n",
    "    \"I'm not sure\",\n",
    "    \"I can't say on that one\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
